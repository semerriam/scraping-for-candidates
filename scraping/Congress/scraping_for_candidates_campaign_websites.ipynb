{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3af860d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4071e826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d292431",
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import random\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f62a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "insta_url='https://www.instagram.com'\n",
    "inta_username= input('enter username of instagram : ')\n",
    " \n",
    "response = requests.get(f\"{insta_url}/{inta_username}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61980d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d04bbce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19a67b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Launch a new Chrome, install the appropriate ChromeDriver if necessary\n",
    "# driver = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f848397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a04ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b3db3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d9fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following should extract all images from a given page and write it to the directory where the script is being run.\n",
    "# https://stackoverflow.com/questions/18408307/how-to-extract-and-download-all-images-from-a-website-using-beautifulsoup\n",
    "\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "site = 'http://pixabay.com'\n",
    "\n",
    "response = requests.get(site)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "img_tags = soup.find_all('img')\n",
    "\n",
    "urls = [img['src'] for img in img_tags]\n",
    "\n",
    "\n",
    "for url in urls:\n",
    "    filename = re.search(r'/([\\w_-]+[.](jpg|gif|png))$', url)\n",
    "    if not filename:\n",
    "         print(\"Regex didn't match with the url: {}\".format(url))\n",
    "         continue\n",
    "    with open(filename.group(1), 'wb') as f:\n",
    "        if 'http' not in url:\n",
    "            # sometimes an image source can be relative \n",
    "            # if it is provide the base url which also happens \n",
    "            # to be the site variable atm. \n",
    "            url = '{}{}'.format(site, url)\n",
    "        response = requests.get(url)\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6589a992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrap-Images-from-Websites-using-BeautifulSoup\n",
    "# https://github.com/nikhilroxtomar/Scrap-Images-from-Websites-using-BeautifulSoup/blob/master/save_images.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54041c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3848b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import cv2\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da2c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import cv2\n",
    "from skimage import io\n",
    "\n",
    "def create_dir(path):\n",
    "    \"\"\" Create folders \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    except OSError:\n",
    "        print(\"Error\")\n",
    "\n",
    "def create_file(path):\n",
    "    \"\"\" Create a file \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            f = open(path, \"w\")\n",
    "            f.write(\"Name,Alt\\n\")\n",
    "            f.close()\n",
    "    except OSError:\n",
    "        print(\"Error\")\n",
    "\n",
    "def save_image(search_term, page_num=1):\n",
    "    ## URL and headers\n",
    "    url = \"https://www.henrycuellar.com/\"\n",
    "    header = {'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36'}\n",
    "\n",
    "    ## making a GET request to the website and getting the information in response.\n",
    "    result = requests.get(url, headers=header)\n",
    "\n",
    "    if result.status_code == 200:\n",
    "        soup = BeautifulSoup(result.content, \"html.parser\")\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "        exit()\n",
    "\n",
    "    ## Paths and file for saving the images and data.\n",
    "    dir_path = f\"Downloads/{search_term}/\"\n",
    "    file_path = f\"Downloads/{search_term}/{search_term}.csv\"\n",
    "\n",
    "    create_dir(dir_path)\n",
    "    create_file(file_path)\n",
    "\n",
    "    f = open(file_path, \"a\")\n",
    "\n",
    "    for tag in soup.find_all(\"a\", class_=\"showcase__link\"):\n",
    "        if tag.img:\n",
    "            try:\n",
    "                src = tag.img[\"data-src\"]\n",
    "                alt = tag.img[\"alt\"]\n",
    "            except Exception as e:\n",
    "                alt = None\n",
    "\n",
    "            try:\n",
    "                if alt:\n",
    "                    image = io.imread(src)\n",
    "                    name = src.split(\"/\")[-1].split(\"?\")[0]\n",
    "                    data = f\"{name},{alt}\\n\"\n",
    "                    f.write(data)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    cv2.imwrite(dir_path + name, image)\n",
    "                    print(name, \": \", alt)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    terms = ['dog', 'cat', 'tree']\n",
    "    for term in terms:\n",
    "        save_image(term, page_num=1)\n",
    "        save_image(term, page_num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f096353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0f90ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from subprocess import Popen, PIPE\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb6e80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from subprocess import Popen, PIPE\n",
    "from selenium import webdriver\n",
    "\n",
    "abspath = lambda *p: os.path.abspath(os.path.join(*p))\n",
    "ROOT = abspath(os.path.dirname(__file__))\n",
    "\n",
    "\n",
    "def execute_command(command):\n",
    "    result = Popen(command, shell=True, stdout=PIPE).stdout.read()\n",
    "    if len(result) > 0 and not result.isspace():\n",
    "        raise Exception(result)\n",
    "\n",
    "\n",
    "def do_screen_capturing(url, screen_path, width, height):\n",
    "    print \"Capturing screen..\"\n",
    "    driver = webdriver.PhantomJS()\n",
    "    # it save service log file in same directory\n",
    "    # if you want to have log file stored else where\n",
    "    # initialize the webdriver.PhantomJS() as\n",
    "    # driver = webdriver.PhantomJS(service_log_path='/var/log/phantomjs/ghostdriver.log')\n",
    "    driver.set_script_timeout(30)\n",
    "    if width and height:\n",
    "        driver.set_window_size(width, height)\n",
    "    driver.get(url)\n",
    "    driver.save_screenshot(screen_path)\n",
    "\n",
    "\n",
    "def do_crop(params):\n",
    "    print \"Croping captured image..\"\n",
    "    command = [\n",
    "        'convert',\n",
    "        params['screen_path'],\n",
    "        '-crop', '%sx%s+0+0' % (params['width'], params['height']),\n",
    "        params['crop_path']\n",
    "    ]\n",
    "    execute_command(' '.join(command))\n",
    "\n",
    "\n",
    "def do_thumbnail(params):\n",
    "    print \"Generating thumbnail from croped captured image..\"\n",
    "    command = [\n",
    "        'convert',\n",
    "        params['crop_path'],\n",
    "        '-filter', 'Lanczos',\n",
    "        '-thumbnail', '%sx%s' % (params['width'], params['height']),\n",
    "        params['thumbnail_path']\n",
    "    ]\n",
    "    execute_command(' '.join(command))\n",
    "\n",
    "\n",
    "def get_screen_shot(**kwargs):\n",
    "    url = kwargs['url']\n",
    "    width = int(kwargs.get('width', 1024)) # screen width to capture\n",
    "    height = int(kwargs.get('height', 768)) # screen height to capture\n",
    "    filename = kwargs.get('filename', 'screen.png') # file name e.g. screen.png\n",
    "    path = kwargs.get('path', ROOT) # directory path to store screen\n",
    "\n",
    "    crop = kwargs.get('crop', False) # crop the captured screen\n",
    "    crop_width = int(kwargs.get('crop_width', width)) # the width of crop screen\n",
    "    crop_height = int(kwargs.get('crop_height', height)) # the height of crop screen\n",
    "    crop_replace = kwargs.get('crop_replace', False) # does crop image replace original screen capture?\n",
    "\n",
    "    thumbnail = kwargs.get('thumbnail', False) # generate thumbnail from screen, requires crop=True\n",
    "    thumbnail_width = int(kwargs.get('thumbnail_width', width)) # the width of thumbnail\n",
    "    thumbnail_height = int(kwargs.get('thumbnail_height', height)) # the height of thumbnail\n",
    "    thumbnail_replace = kwargs.get('thumbnail_replace', False) # does thumbnail image replace crop image?\n",
    "\n",
    "    screen_path = abspath(path, filename)\n",
    "    crop_path = thumbnail_path = screen_path\n",
    "\n",
    "    if thumbnail and not crop:\n",
    "        raise Exception, 'Thumnail generation requires crop image, set crop=True'\n",
    "\n",
    "    do_screen_capturing(url, screen_path, width, height)\n",
    "\n",
    "    if crop:\n",
    "        if not crop_replace:\n",
    "            crop_path = abspath(path, 'crop_'+filename)\n",
    "        params = {\n",
    "            'width': crop_width, 'height': crop_height,\n",
    "            'crop_path': crop_path, 'screen_path': screen_path}\n",
    "        do_crop(params)\n",
    "\n",
    "        if thumbnail:\n",
    "            if not thumbnail_replace:\n",
    "                thumbnail_path = abspath(path, 'thumbnail_'+filename)\n",
    "            params = {\n",
    "                'width': thumbnail_width, 'height': thumbnail_height,\n",
    "                'thumbnail_path': thumbnail_path, 'crop_path': crop_path}\n",
    "            do_thumbnail(params)\n",
    "    return screen_path, crop_path, thumbnail_path\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "        Requirements:\n",
    "        Install NodeJS\n",
    "        Using Node's package manager install phantomjs: npm -g install phantomjs\n",
    "        install selenium (in your virtualenv, if you are using that)\n",
    "        install imageMagick\n",
    "        add phantomjs to system path (on windows)\n",
    "    '''\n",
    "\n",
    "    url = 'http://stackoverflow.com/questions/1197172/how-can-i-take-a-screenshot-image-of-a-website-using-python'\n",
    "    screen_path, crop_path, thumbnail_path = get_screen_shot(\n",
    "        url=url, filename='sof.png',\n",
    "        crop=True, crop_replace=False,\n",
    "        thumbnail=True, thumbnail_replace=False,\n",
    "        thumbnail_width=200, thumbnail_height=150,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a1e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "site = 'http://pixabay.com'\n",
    "\n",
    "response = requests.get(site)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "img_tags = soup.find_all('img')\n",
    "\n",
    "urls = [img['src'] for img in img_tags]\n",
    "\n",
    "\n",
    "for url in urls:\n",
    "    filename = re.search(r'/([\\w_-]+[.](jpg|gif|png))$', url)\n",
    "    if not filename:\n",
    "         print(\"Regex didn't match with the url: {}\".format(url))\n",
    "         continue\n",
    "    with open(filename.group(1), 'wb') as f:\n",
    "        if 'http' not in url:\n",
    "            # sometimes an image source can be relative \n",
    "            # if it is provide the base url which also happens \n",
    "            # to be the site variable atm. \n",
    "            url = '{}{}'.format(site, url)\n",
    "        response = requests.get(url)\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce82368c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234bc42a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5b2b4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "  \n",
    "list = ['folder10','folder11','folder12',\n",
    "        'folder13', 'folder15']\n",
    "  \n",
    "for items in list:\n",
    "    os.mkdir(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfc42d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceac166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fac3fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"outputs//output_house_candidate_campaign_links.csv\", encoding=\"utf-8\")\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17136a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4432ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to extract all images on a website\n",
    "\n",
    "# https://stackoverflow.com/questions/18408307/how-to-extract-and-download-all-images-from-a-website-using-beautifulsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af427ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# site = 'http://www.google.com'\n",
    "\n",
    "# response = requests.get(site)\n",
    "\n",
    "# soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# img_tags = soup.find_all('img')\n",
    "\n",
    "# urls = [img['src'] for img in img_tags]\n",
    "\n",
    "\n",
    "# for url in urls:\n",
    "#     filename = re.search(r'/([\\w_-]+[.](jpg|gif|png))$', url)\n",
    "#     if not filename:\n",
    "#          print(\"Regex didn't match with the url: {}\".format(url))\n",
    "#          continue\n",
    "#     with open(filename.group(1), 'wb') as f:\n",
    "#         if 'http' not in url:\n",
    "#             # sometimes an image source can be relative \n",
    "#             # if it is provide the base url which also happens \n",
    "#             # to be the site variable atm. \n",
    "#             url = '{}{}'.format(site, url)\n",
    "#         response = requests.get(url)\n",
    "#         f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a9b25d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5e34ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://betterprogramming.pub/how-to-easily-scrape-multiple-pages-of-a-website-using-python-73e85bd06f8c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57058306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting data in a txt file\n",
    "\n",
    "# with open(f'{title}.txt', 'w') as file:\n",
    "#     file.write(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13deea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aec960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b13de20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a6af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/adityamangal1/Image-Scraping-Python/blob/master/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1c2200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bs4\n",
    "# import requests\n",
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# GOOGLE_IMAGE = \\\n",
    "#     'https://www.google.com/search?site=&tbm=isch&source=hp&biw=1873&bih=990&'\n",
    "\n",
    "\n",
    "# def extract(data, quantity):\n",
    "#     URL_input = GOOGLE_IMAGE + 'q=' + data\n",
    "#     print('Fetching from url =', URL_input)\n",
    "#     URLdata = requests.get(URL_input)\n",
    "#     soup = bs4.BeautifulSoup(URLdata.text, \"html.parser\")\n",
    "#     ImgTags = soup.find_all('img')\n",
    "#     i = 0\n",
    "#     print('Please wait..')\n",
    "#     while i < quantity:\n",
    "\n",
    "#         for link in ImgTags:\n",
    "#             try:\n",
    "#                 images = link.get('src')\n",
    "#                 ext = images[images.rindex('.'):]\n",
    "#                 if ext.startswith('.png'):\n",
    "#                     ext = '.png'\n",
    "#                 elif ext.startswith('.jpg'):\n",
    "#                     ext = '.jpg'\n",
    "#                 elif ext.startswith('.jfif'):\n",
    "#                     ext = '.jfif'\n",
    "#                 elif ext.startswith('.com'):\n",
    "#                     ext = '.jpg'\n",
    "#                 elif ext.startswith('.svg'):\n",
    "#                     ext = '.svg'\n",
    "#                 data = requests.get(images, stream=True)\n",
    "#                 filename = str(i) + ext\n",
    "#                 with open(filename, 'wb') as file:\n",
    "#                     shutil.copyfileobj(data.raw, file)\n",
    "#                 i += 1\n",
    "#             except:\n",
    "#                 pass\n",
    "#     print('\\t\\t\\t Downloaded Successfully..\\t\\t ')\n",
    "\n",
    "\n",
    "# data = input('What are you looking for? ')\n",
    "# quantity = int(input('How many photos you want? '))\n",
    "# extract(data, quantity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72435838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/nikhilroxtomar/Scrap-Images-from-Websites-using-BeautifulSoup/blob/master/save_images.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
